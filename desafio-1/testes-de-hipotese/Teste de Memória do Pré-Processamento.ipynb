{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abf6bb9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> \n",
    "        Teste de Memória do Pré-Processamento\n",
    "    </h1>\n",
    "</center>\n",
    "    <div style=\"text-align: right;\"><h3>\n",
    "        Carlos Eduardo Cassimiro da Silva\n",
    "    </h3></div>\n",
    "\n",
    "<center>\n",
    "    <h4>Neste notebook, iremos começar o pré-processamento do conjunto de dados original, pois até então os testes estavam sendo feitos com uma amostra do original. Também verificaremos o quanto podemos reduzir de espaço o conjuto de dados original transformando as imagens em histogramas.</h4>\n",
    "    Link do dataset: https://github.com/ricardobnjunior/Brazilian-Identity-Document-Dataset\n",
    "</center>\n",
    "\n",
    "<h4>Desafio #1: Classificação de documentos (RG, CNH e CPF) </h4>\n",
    "\n",
    " - Contextualização: Inúmeras áreas de diferentes organizações (usualmente como parte de um processo de um backoffice) recepcionam documentos dos seus clientes para formação de kits de documentação. Tais kits são, por exemplo, compartilhados com outros stakeholders das empresas. Conforme pode-se pressupor, um desafio nesse cenário refere-se ao fato de que o cliente pode enviar documentos, porém sem necessariamente indicar a qual tipo se o documento se refere (RG, CNH ou CPF, por exemplo). Dessa forma, ao invés de demandar um trabalho manual para essa leitura e classificação dos documentos, podemos construir um modelo de aprendizado que tenha capacidade de ler um conjunto de documentos (em .jpg, por exemplo) e, subsequentemente, realizar a classificação em três tipos distintos: RG, CNH e CPF.\n",
    " - Dataset: Para esse desafio utilizaremos um dataset público de RG, CNH e CPF (incluindo as imagens). Este repositório apresenta o conjunto de dados denominado Brazilian Identity Document Dataset (BID Dataset), o primeiro conjunto de dados público de documentos de identificação brasileiros. <br>\n",
    "\n",
    "<h4>Roteiro</h4>\n",
    "Módulos utilizados <br>\n",
    "1. Abrindo todas as amostras de um tipo de documento <br>\n",
    "1.1 Somatório do espaço ocupado <br>\n",
    "2. Transformando as amostras em histogramas <br>\n",
    "2.1 Somatório do espaço ocupado <br>\n",
    "3. Economia <br>\n",
    "4. Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469dbd3",
   "metadata": {},
   "source": [
    "##### Módulos utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d53633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d7573",
   "metadata": {},
   "source": [
    "# 1. Abrindo todas as amostras de um tipo de documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858939c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'imagens_originais/BID_Dataset/CPF_Frente/' # caminho do diretório\n",
    "a = os.listdir(path) # os.listdir lista todos os arquivos ou pastas dentro do referido diretório \n",
    "doc_list = [] # lista para guardar as amostras\n",
    "for i in range(2,len(a),3): # Laço para pegar os documentos. Como queremos só as imagens dos documentos,\n",
    "    doc_list.append(cv2.imread(path+a[i]))                                  # pulamos os outros arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd37a4",
   "metadata": {},
   "source": [
    "## 1.1 Somatório do espaço ocupado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5052d6",
   "metadata": {},
   "source": [
    "Para examinar o valor do espaço ocupado em memória, precisamos utilizar o 'getsizeof' para descobrir o valor ocupado por cada variável e multiplicar pelo valor de cada lista que as comportam. Como se trata de imagem, precisamos executar do nível mais baixo da estrutura de dados até o mais alto, como espaço de cada pixel, número de pixel, numero de linhas e multiplar pelo número e espaço ocupado por cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2f0537a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Espaço ocupado: 1.763122363947332 GB'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma = getsizeof(doc_list[0][0][0][0])*len(doc_list[0][0][0])\n",
    "soma += getsizeof(doc_list[0][0][0])\n",
    "soma *= len(doc_list[0][0][0])\n",
    "soma += getsizeof(doc_list[0][0])\n",
    "\n",
    "aux = 0\n",
    "for i in range(len(doc_list)):\n",
    "    aux += len(doc_list[i][0])\n",
    "\n",
    "soma *= aux\n",
    "soma += (getsizeof(doc_list[i])*len(doc_list[i]))\n",
    "\n",
    "soma += getsizeof(doc_list)\n",
    "soma /= (1024*1024*1024)\n",
    "'Espaço ocupado: '+ str(soma) + ' GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da5b82",
   "metadata": {},
   "source": [
    "# 2. Transformando as amostras em histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40656a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ksize = (70,70)\n",
    "#cv2.cvtColor(), cv2.COLOR_BGR2RGB\n",
    "#cv2.blur(img_aux, ksize)\n",
    "\n",
    "doc = copy.deepcopy(doc_list)\n",
    "\n",
    "for i in range(len(doc_list)):\n",
    "    hist = [] \n",
    "    \n",
    "    hist.append(cv2.calcHist(doc[i],[0],None,[256],[0,256]))  # Processo análogo ao anterior para \n",
    "    hist.append(cv2.calcHist(doc[i],[1],None,[256],[0,256]))  # cálcular as média dos histogramas\n",
    "    hist.append(cv2.calcHist(doc[i],[2],None,[256],[0,256]))\n",
    "    for j in range(0,3):\n",
    "        mm = MinMaxScaler()\n",
    "        hist[j] = mm.fit_transform(hist[j], hist[j])\n",
    "    doc[i] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79959d75",
   "metadata": {},
   "source": [
    "## 2.1 Somatório do espaço ocupado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3cd47b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Espaço ocupado: 103.51992797851562 MB'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma = getsizeof(doc[0][0][0])*len(doc[0][0])\n",
    "soma += getsizeof(doc[0][0])*3\n",
    "soma += getsizeof(doc[0])\n",
    "soma *= len(doc)\n",
    "soma += getsizeof(doc)\n",
    "soma /= (1024*1024)\n",
    "'Espaço ocupado: '+ str(soma) + ' MB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae78cb0",
   "metadata": {},
   "source": [
    "# 3. Economia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9b601e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economia de 94.26621306985254 %\n"
     ]
    }
   ],
   "source": [
    "print('Economia de' ,100-(100*(103.51992797851562/1024)/1.763122363947332), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177f46a",
   "metadata": {},
   "source": [
    "# 4. Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b278b",
   "metadata": {},
   "source": [
    "Podemos ver que, adatodando a estratégia de utilizar os histogramas ao invés das imagens originais, conseguimos reduzir em 94% o espaço ocupado do CPF_Frente, mas acredito que podemos generalizar esse valor para os outros documentos também. Reduzindo o valor nessa proporção, acredito que podemos utilizar todo o dataset sem problemas de memória."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
